<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Geometry-Aware Explainable Fraud Detection</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <!-- Adding Inter font for a more polished academic look -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <style>
    body { font-family: 'Inter', sans-serif; }
  </style>
</head>

<body class="bg-gray-50 text-gray-900 leading-relaxed">

<!-- HERO -->
<section class="max-w-5xl mx-auto px-6 py-20">
  <h1 class="text-4xl font-bold mb-6 tracking-tight">
    When Better Fraud Detection Becomes Less Explainable
  </h1>
  <p class="text-xl text-gray-700 max-w-3xl">
    This research shows that common imbalance-handling strategies in deep generative
    fraud detection models can <strong>improve detection performance while
    systematically destroying explainability</strong>.
  </p>
  <p class="mt-6 text-gray-600 max-w-3xl">
    The failure does not originate in explainability methods such as SHAP or attention.
    It originates earlier — in the geometry of learned latent representations.
  </p>
</section>

<!-- THE PARADOX -->
<section class="bg-white border-t border-b border-gray-200">
  <div class="max-w-5xl mx-auto px-6 py-16">
    <h2 class="text-2xl font-semibold mb-4 text-gray-900">
      The Central Paradox
    </h2>
    <p class="text-gray-700 max-w-3xl mb-4">
      In highly imbalanced domains such as credit card fraud detection, practitioners
      routinely introduce imbalance-aware objectives to improve minority detection.
      Metrics such as AUPRC improve, recall increases, and models appear operationally stronger.
    </p>
    <p class="text-gray-700 max-w-3xl mb-4">
      However, this work demonstrates a paradoxical effect:
    </p>
    <div class="bg-red-50 border-l-4 border-red-700 p-4 max-w-3xl">
      <p class="text-lg font-medium text-red-900">
        As imbalance-aware generative models perform better, their internal representations
        collapse — making explanations increasingly unreliable.
      </p>
    </div>
  </div>
</section>

<!-- WHAT GOES WRONG -->
<section class="max-w-5xl mx-auto px-6 py-16">
  <h2 class="text-2xl font-semibold mb-6">
    What Goes Wrong in Imbalance-Aware VAEs
  </h2>

  <ol class="list-decimal list-inside space-y-4 text-gray-700 max-w-3xl pl-2">
    <li class="pl-2">
      <span class="font-medium text-gray-900">Objective-level imbalance handling</span> scales loss terms to emphasise minority samples.
    </li>
    <li class="pl-2">
      In Variational Autoencoders, this scaling <span class="font-medium text-gray-900">amplifies gradient variance</span> acting on
      latent variance parameters.
    </li>
    <li class="pl-2">
      Under extreme imbalance, this induces <span class="font-medium text-gray-900">systematic shrinkage</span> of minority latent covariance.
    </li>
    <li class="pl-2">
      The minority latent manifold <span class="font-medium text-gray-900">collapses</span> into a low-rank, near-singular structure.
    </li>
    <li class="pl-2">
      Explainability methods operating on these representations <span class="font-medium text-gray-900">lose discriminative power</span>.
    </li>
  </ol>

  <p class="mt-8 text-gray-600 max-w-3xl italic border-l-2 border-gray-300 pl-4">
    Crucially, this collapse occurs even when detection metrics improve.
    The issue is therefore representational — not predictive.
  </p>
</section>

<!-- NEGATIVE RESULT -->
<section class="bg-red-50 border-t border-b border-red-200">
  <div class="max-w-5xl mx-auto px-6 py-16">
    <h2 class="text-2xl font-semibold mb-4 text-red-800">
      A Negative Result the Field Has Missed
    </h2>
    <p class="text-gray-800 max-w-3xl mb-4">
      This work provides theoretical and empirical evidence that
      <strong>ratio-weighted KL objectives in Balanced VAEs induce minority latent collapse
      as a direct consequence of gradient variance amplification</strong>.
    </p>
    <p class="text-gray-700 max-w-3xl">
      The collapse is predictable, systematic, and reproducible across datasets.
      It is not an optimisation accident, nor a failure of explainability algorithms.
    </p>
  </div>
</section>

<!-- WHY XAI FAILS -->
<section class="max-w-5xl mx-auto px-6 py-16">
  <h2 class="text-2xl font-semibold mb-6">
    Why Explainability Degrades
  </h2>
  <p class="text-gray-700 max-w-3xl mb-6">
    Both intrinsic and post hoc explainability methods assume that internal representations
    are expressive, high-rank, and locally informative.
  </p>

  <div class="grid md:grid-cols-3 gap-6 max-w-4xl">
    <div class="bg-white p-6 rounded shadow-sm border border-gray-100">
      <h3 class="font-semibold text-gray-900 mb-2">Latent Geometry</h3>
      <p class="text-sm text-gray-600">Collapsed geometry produces uniform gradients, stripping vectors of directionality.</p>
    </div>
    <div class="bg-white p-6 rounded shadow-sm border border-gray-100">
      <h3 class="font-semibold text-gray-900 mb-2">Attention</h3>
      <p class="text-sm text-gray-600">Mechanisms lose entropy and become degenerate, attending to everything or nothing.</p>
    </div>
    <div class="bg-white p-6 rounded shadow-sm border border-gray-100">
      <h3 class="font-semibold text-gray-900 mb-2">SHAP</h3>
      <p class="text-sm text-gray-600">Attributions converge toward near-identical explanations regardless of input features.</p>
    </div>
  </div>

  <p class="mt-8 text-gray-600 max-w-3xl font-medium">
    Explanations may appear stable — but they are no longer faithful.
  </p>
</section>

<!-- GEOMETRY-AWARE SOLUTION -->
<section class="bg-white border-t border-gray-200">
  <div class="max-w-5xl mx-auto px-6 py-16">
    <h2 class="text-2xl font-semibold mb-6">
      Geometry-Aware Regularisation
    </h2>
    <p class="text-gray-700 max-w-3xl mb-6">
      Rather than scaling loss terms by the imbalance ratio, this research introduces
      a geometry-aware regularisation strategy based on Mahalanobis distance.
    </p>

    <div class="flex flex-col md:flex-row gap-8 max-w-4xl mb-8">
      <div class="flex-1">
        <h3 class="text-sm uppercase tracking-wide text-gray-500 font-bold mb-2">Traditional Approach</h3>
        <p class="text-gray-800">Loss reweighting amplifies gradients.</p>
      </div>
      <div class="hidden md:block w-px bg-gray-200"></div>
      <div class="flex-1">
        <h3 class="text-sm uppercase tracking-wide text-blue-600 font-bold mb-2">Geometry-Aware</h3>
        <p class="text-gray-800">Regularisation constrains latent structure directly.</p>
      </div>
    </div>

    <p class="text-gray-600 max-w-3xl">
      This preserves minority covariance, maintains effective latent rank,
      and restores explainability — without sacrificing detection performance.
    </p>
  </div>
</section>

<!-- WHAT THIS CHANGES -->
<section class="bg-gray-100 border-t border-gray-200">
  <div class="max-w-5xl mx-auto px-6 py-16">
    <h2 class="text-2xl font-semibold mb-6">
      What This Changes
    </h2>

    <ul class="space-y-4 text-gray-700 max-w-3xl">
      <li class="flex items-start">
        <svg class="w-6 h-6 text-green-600 mr-3 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
        <span>Detection performance alone is insufficient for evaluating imbalanced models.</span>
      </li>
      <li class="flex items-start">
        <svg class="w-6 h-6 text-green-600 mr-3 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
        <span>Explainability cannot be repaired post hoc if representations collapse.</span>
      </li>
      <li class="flex items-start">
        <svg class="w-6 h-6 text-green-600 mr-3 flex-shrink-0" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg>
        <span>Imbalance handling must be geometry-aware, not gradient-amplifying.</span>
      </li>
    </ul>

    <p class="mt-8 text-gray-800 font-medium max-w-3xl">
      Explainability in imbalanced learning is fundamentally a representational problem.
    </p>
  </div>
</section>

<!-- FOOTER -->
<footer class="max-w-5xl mx-auto px-6 py-12 text-sm text-gray-500 border-t border-gray-200">
  <p>
    This site provides a conceptual overview of the accompanying research thesis.
    For theoretical derivations, experimental methodology, and full empirical results,
    refer to the dissertation.
  </p>
</footer>

</body>
</html>

